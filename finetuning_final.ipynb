{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\warpspace\\.conda\\envs\\nlp118\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\warpspace\\.conda\\envs\\nlp118\\Lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\warpspace\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = 'https://raw.githubusercontent.com/NLP7-LegalEagle/LegalEagle-Dataset/main/instruction_datasets/dataset_train.csv'\n",
    "val_url = 'https://raw.githubusercontent.com/NLP7-LegalEagle/LegalEagle-Dataset/main/instruction_datasets/dataset_validation.csv'\n",
    "\n",
    "train_dataset = load_dataset('csv', data_files={\"train\": train_url})\n",
    "val_dataset = load_dataset('csv', data_files={\"validation\": val_url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c0337428a244239c61d19c45bb1575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create 4-bit quantization with NF4 type configuration using BitsAndBytes\n",
    "base_model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# load base model\n",
    "\n",
    "device_map = {\"\": 0} # \"cuda:0\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    #use_auth_token=True\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name, \n",
    "    padding=True,\n",
    "    truncation=True, \n",
    "    max_length=512,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"  # to fix the issye with fp16\n",
    "\n",
    "output_dir = \"./results07B-chat-8\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    max_steps=5,\n",
    "    save_safetensors=True,\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    #eval_steps=100,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    #save_steps=5, \n",
    "    #save_total_limit=3,\n",
    ")\n",
    "\n",
    "max_seq_length = 1024\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    eval_dataset=val_dataset['validation'],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"result\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cc8a643914462091d91d282c2d59a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3375, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 1.3757, 'learning_rate': 0.00017777777777777779, 'epoch': 0.0}\n",
      "{'loss': 1.2861, 'learning_rate': 0.00015555555555555556, 'epoch': 0.0}\n",
      "{'loss': 1.2911, 'learning_rate': 0.00013333333333333334, 'epoch': 0.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=\"./results07B-chat-5/checkpoint-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f10f7b956b46e5a8bfe47915bb7ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3102, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 1.3465, 'learning_rate': 0.00015000000000000001, 'epoch': 0.0}\n",
      "{'loss': 1.2666, 'learning_rate': 0.0001, 'epoch': 0.0}\n",
      "{'loss': 1.2758, 'learning_rate': 5e-05, 'epoch': 0.0}\n",
      "{'loss': 1.2868, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'train_runtime': 2898.7547, 'train_samples_per_second': 0.055, 'train_steps_per_second': 0.002, 'train_loss': 1.297163724899292, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=1.297163724899292, metrics={'train_runtime': 2898.7547, 'train_samples_per_second': 0.055, 'train_steps_per_second': 0.002, 'train_loss': 1.297163724899292, 'epoch': 0.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=\"./results07B-chat-7/checkpoint-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]  was acting to obtain a benefit on behalf of a charitable ... organization.” U.S.S.G. § 2B1.1 cmt. 8(B). As the district court saw it and as the government sees it, Webster deserves the enhancement. He pretended to “act[ ] on behalf of a charitable ... organization,” U.S.S.G. § 2Bl.l(b)(9)(A), when he solicited personal information from the victims on behalf of fake charities. As Webster sees it, the enhancement does not apply. In his view, the commentary limits the application of the charity enhancement, and he was not acting to obtain a benefit on behalf of a charitable organization (as the commentary seems to require). As a general matter, the text of a guideline trumps commentary about it. See Stinson v. United States, 508 U.S. 36, 38, 113 S.Ct. 1913, 123 L.Ed.2d 598 (1993) (<HOLDING>). But we need not resolve whether the [/INST]  issue is whether the district court correctly applied the enhancement for obstruction of justice under U.S.S.G. § 2B1.1. The government argues that Webster's actions meet the criteria for the enhancement, as he pretended to act on behalf of a charitable organization to obtain personal information from victims. Webster, on the other hand, argues that the commentary to the guideline limits the application of the enhancement and that he was not acting to obtain a benefit on behalf of a charitable organization.\n",
      "\n",
      "The issue of whether the enhancement applies is a question of fact, and the district court's determination is reviewed for clear error. United States v. Rivera, 778 F.3d 118, 124 (1st Cir. 2015). The government must prove by a preponderance of the evidence that the defendant committed the obstruction of justice offense. Id.\n",
      "\n",
      "The commentary to U.S.S.G. § 2B1.1 provides that the enhancement applies when the defendant \"pretended to act on behalf of a charitable organization, or pretended to have some other legitimate purpose, in order to obstruct the investigation or prosecution of the offense.\" U.S.S.G. § 2B1.1 cmt. 8(B). The commentary also provides that the enhancement does not apply when the defendant's actions were not intended to obstruct the investigation or prosecution of the offense. Id.\n",
      "\n",
      "In this case, the district court found that Webster pretended to act on behalf of a charitable organization to obtain personal information from victims, and that he did so with the intent to obstruct the investigation or prosecution of the offense. The district court therefore correctly applied the obstruction of justice enhancement under U.S.S.G. § 2B1.1.\n",
      "\n",
      "Webster argues that the commentary to the guideline limits the application of the enhancement and that he was not acting to obtain a benefit on behalf of a charitable organization. However, the commentary does not provide an exemption for defendants who pretend to act on behalf of a charitable organization, but rather provides that the enhancement does not apply when the defendant's actions were not intended to obstruct the investigation or prosecution\n"
     ]
    }
   ],
   "source": [
    "#fine-tuning 전 base_model\n",
    "\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \" was acting to obtain a benefit on behalf of a charitable ... organization.” U.S.S.G. § 2B1.1 cmt. 8(B). As the district court saw it and as the government sees it, Webster deserves the enhancement. He pretended to “act[ ] on behalf of a charitable ... organization,” U.S.S.G. § 2Bl.l(b)(9)(A), when he solicited personal information from the victims on behalf of fake charities. As Webster sees it, the enhancement does not apply. In his view, the commentary limits the application of the charity enhancement, and he was not acting to obtain a benefit on behalf of a charitable organization (as the commentary seems to require). As a general matter, the text of a guideline trumps commentary about it. See Stinson v. United States, 508 U.S. 36, 38, 113 S.Ct. 1913, 123 L.Ed.2d 598 (1993) (<HOLDING>). But we need not resolve whether the\"\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=800)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "#FINprint(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]  was acting to obtain a benefit on behalf of a charitable ... organization.” U.S.S.G. § 2B1.1 cmt. 8(B). As the district court saw it and as the government sees it, Webster deserves the enhancement. He pretended to “act[ ] on behalf of a charitable ... organization,” U.S.S.G. § 2Bl.l(b)(9)(A), when he solicited personal information from the victims on behalf of fake charities. As Webster sees it, the enhancement does not apply. In his view, the commentary limits the application of the charity enhancement, and he was not acting to obtain a benefit on behalf of a charitable organization (as the commentary seems to require). As a general matter, the text of a guideline trumps commentary about it. See Stinson v. United States, 508 U.S. 36, 38, 113 S.Ct. 1913, 123 L.Ed.2d 598 (1993) (<HOLDING>). But we need not resolve whether the [/INST]  The issue in this case is whether the defendant, Webster, deserves an enhancement under U.S.S.G. § 2B1.1 for acting to obtain a benefit on behalf of a charitable organization. The government argues that Webster does deserve the enhancement because he pretended to act on behalf of fake charities and solicited personal information from victims. Webster, on the other hand, argues that the enhancement does not apply because he was not acting to obtain a benefit on behalf of a charitable organization as required by the commentary to § 2B1.1.\n",
      "\n",
      "The district court saw it and the government sees it this way: Webster pretended to act on behalf of charitable organizations when he solicited personal information from victims, and therefore he deserves the enhancement. However, Webster sees it differently: he argues that the commentary to § 2B1.1 limits the application of the charity enhancement, and he was not acting to obtain a benefit on behalf of a charitable organization as required by the commentary.\n",
      "\n",
      "As a general matter, the text of a guideline trumps commentary about it. See Stinson v. United States, 508 U.S. 36, 38, 113 S.Ct. 1913, 123 L.Ed.2d 598 (1993) (holding). However, in this case, we need not resolve whether the enhancement applies because the issue is not critical to the outcome of the case. The district court's decision to apply the enhancement was based on its interpretation of the guideline and the evidence presented at trial, and that decision is not being challenged on appeal. Therefore, we will not address the issue of whether the enhancement applies in this case.\n"
     ]
    }
   ],
   "source": [
    "#fine-tuning 후\n",
    "\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \" was acting to obtain a benefit on behalf of a charitable ... organization.” U.S.S.G. § 2B1.1 cmt. 8(B). As the district court saw it and as the government sees it, Webster deserves the enhancement. He pretended to “act[ ] on behalf of a charitable ... organization,” U.S.S.G. § 2Bl.l(b)(9)(A), when he solicited personal information from the victims on behalf of fake charities. As Webster sees it, the enhancement does not apply. In his view, the commentary limits the application of the charity enhancement, and he was not acting to obtain a benefit on behalf of a charitable organization (as the commentary seems to require). As a general matter, the text of a guideline trumps commentary about it. See Stinson v. United States, 508 U.S. 36, 38, 113 S.Ct. 1913, 123 L.Ed.2d 598 (1993) (<HOLDING>). But we need not resolve whether the\"\n",
    "pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_length=800)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b111a15c81e480cad6d96970a5a4f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('07b_chat_finetune_m8\\\\tokenizer_config.json',\n",
       " '07b_chat_finetune_m8\\\\special_tokens_map.json',\n",
       " '07b_chat_finetune_m8\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "trained_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir\n",
    ")\n",
    "\n",
    "merged_model = trained_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"07b_chat_finetune_m8\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"07b_chat_finetune_m8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]  was acting to obtain a benefit on behalf of a charitable ... organization.” U.S.S.G. § 2B1.1 cmt. 8(B). As the district court saw it and as the government sees it, Webster deserves the enhancement. He pretended to “act[ ] on behalf of a charitable ... organization,” U.S.S.G. § 2Bl.l(b)(9)(A), when he solicited personal information from the victims on behalf of fake charities. As Webster sees it, the enhancement does not apply. In his view, the commentary limits the application of the charity enhancement, and he was not acting to obtain a benefit on behalf of a charitable organization (as the commentary seems to require). As a general matter, the text of a guideline trumps commentary about it. See Stinson v. United States, 508 U.S. 36, 38, 113 S.Ct. 1913, 123 L.Ed.2d 598 (1993) (<HOLDING>). But we need not resolve whether the [/INST]  holding that the commentary to the sentencing guidelines is not entitled to the same weight as the text of the guidelines when interpreting the guidelines\n",
      "\n",
      "The defendant, Webster, was convicted of wire fraud and aggravated identity theft. The government moved for an enhancement under U.S.S.G. § 2B1.1(b)(9)(A), which provides for an enhancement if the defendant was acting to obtain a benefit on behalf of a charitable organization. The district court granted the enhancement, and Webster appealed.\n",
      "\n",
      "The parties agree that the commentary to § 2B1.1(b)(9)(A) provides that the enhancement applies if the defendant was acting to obtain a benefit on behalf of a charitable organization. However, Webster argues that the commentary does not apply because he was not acting on behalf of a charitable organization. The government argues that the commentary is not dispositive and that the enhancement applies regardless of whether Websterohl act on behalf of a charitable organization.\n",
      "\n",
      "The court of appeals held that the commentary to § 2B1.1(b)(9)(A) is not dispositive and that the enhancement applies regardless of whether the defendant was acting on behalf of a charitable organization. The court of appeals noted that the text of the guidelines trumps the commentary and that the text does not require that the defendant be acting on behalf of a charitable organization. The court of appeals also noted that the government’s interpretation of the guidelines is reasonable and that the district court did not err in granting the enhancement.\n",
      "\n",
      "The Supreme Court held that the commentary to § 2B1.1(b)(9)(A) is not entitled to the same weight as the text of the guidelines when interpreting the guidelines. The Court noted that the commentary is not part of the guidelines and that it is not entitled to the same weight as the text of the guidelines. The Court also noted that the text of the guidelines is the starting point for interpreting the guidelines and that the commentary is only persuasive authority.\n",
      "\n",
      "The Supreme Court also held that the district court did not err in granting the enhancement. The Court noted that the text of the guidelines does not require that the defendant be\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \" was acting to obtain a benefit on behalf of a charitable ... organization.” U.S.S.G. § 2B1.1 cmt. 8(B). As the district court saw it and as the government sees it, Webster deserves the enhancement. He pretended to “act[ ] on behalf of a charitable ... organization,” U.S.S.G. § 2Bl.l(b)(9)(A), when he solicited personal information from the victims on behalf of fake charities. As Webster sees it, the enhancement does not apply. In his view, the commentary limits the application of the charity enhancement, and he was not acting to obtain a benefit on behalf of a charitable organization (as the commentary seems to require). As a general matter, the text of a guideline trumps commentary about it. See Stinson v. United States, 508 U.S. 36, 38, 113 S.Ct. 1913, 123 L.Ed.2d 598 (1993) (<HOLDING>). But we need not resolve whether the\"\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer, max_length=800)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ggoseto/LEModel_2\n",
    "epoch = 3\n",
    "step = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/ggoseto/LEModel_2', endpoint='https://huggingface.co', repo_type='model', repo_id='ggoseto/LEModel_2')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "create_repo(\"ggoseto/LEModel_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\warpspace\\.conda\\envs\\nlp118\\Lib\\site-packages\\transformers\\utils\\hub.py:831: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8bcc10665d489eb99a319679ceaa4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f5dff277484da2945b4cf7395163ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/7.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805a3e45b6dd46f5b820b53630f61406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3407fd191a7497792a30cc66bdc5a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\warpspace\\.conda\\envs\\nlp118\\Lib\\site-packages\\transformers\\utils\\hub.py:831: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ggoseto/LEModel_2/commit/05938e035bf79082c796358aa5e845ecc411e2e0', commit_message='Upload tokenizer', commit_description='', oid='05938e035bf79082c796358aa5e845ecc411e2e0', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TrainOutput(global_step=5, training_loss=1.297163724899292, 'train_loss': 1.297163724899292, 'epoch': 0.0})\n",
    "\"\"\"\n",
    "\n",
    "AUTH_TOKEN = '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'\n",
    "\n",
    "# Repository 생성 & model upload\n",
    "REPO_NAME = 'ggoseto/LEModel_2' \n",
    "AUTH_TOKEN = AUTH_TOKEN \n",
    " \n",
    "## Upload to Huggingface Hub\n",
    "merged_model.push_to_hub(\n",
    "    REPO_NAME, \n",
    "    use_temp_dir=True, \n",
    "    use_auth_token=AUTH_TOKEN\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    REPO_NAME, \n",
    "    use_temp_dir=True, \n",
    "    use_auth_token=AUTH_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ggoseto/LEModel_3\n",
    "epoch = 3\n",
    "step = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebd3876ba124fcbb94a80f140bd34bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "\"\"\"\n",
    "{'loss': 1.2911, 'learning_rate': 0.00013333333333333334, 'epoch': 0.0}\n",
    "(result07B-chat-5:checkpoint-25) + 5\n",
    "epoch= 3, step = 30\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "output_dir = \"./results07B-chat-7/checkpoint-5\"\n",
    "#trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "trained_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('07b_chat_finetune_m7\\\\tokenizer_config.json',\n",
       " '07b_chat_finetune_m7\\\\special_tokens_map.json',\n",
       " '07b_chat_finetune_m7\\\\tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model = trained_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"07b_chat_finetune_m7\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"07b_chat_finetune_m7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]  was acting to obtain a benefit on behalf of a charitable ... organization.” U.S.S.G. § 2B1.1 cmt. 8(B). As the district court saw it and as the government sees it, Webster deserves the enhancement. He pretended to “act[ ] on behalf of a charitable ... organization,” U.S.S.G. § 2Bl.l(b)(9)(A), when he solicited personal information from the victims on behalf of fake charities. As Webster sees it, the enhancement does not apply. In his view, the commentary limits the application of the charity enhancement, and he was not acting to obtain a benefit on behalf of a charitable organization (as the commentary seems to require). As a general matter, the text of a guideline trumps commentary about it. See Stinson v. United States, 508 U.S. 36, 38, 113 S.Ct. 1913, 123 L.Ed.2d 598 (1993) (<HOLDING>). But we need not resolve whether the [/INST]  holding that the commentary in USSG § 2B1.1(b)(9)(A) is not entitled to Chevron deference because it is not a binding rule and the court should look to the text of the guideline in determining its meaning.\n",
      "\n",
      "In this case, the defendant argues that the commentary in USSG § 2B1.1(b)(9)(A) does not apply because he was not acting on behalf of a charitable organization. The government, on the other hand, argues that the commentary is binding and requires the application of the enhancement. The district court held that the commentary is not binding and that the enhancement does not apply.\n",
      "\n",
      "The Supreme Court has held that the text of a guideline trumps commentary about it. Stinson v. United States, 508 U.S. 36, 38, 113 S.Ct. 1913, 123 L.Ed.2d 598 (1993). However, the Court has also held that commentary is entitled to Chevron deference if it is a binding rule. Id. at 42-43, 113 Conseil d'Etat, 123 L.Ed.2d 598 (<HOLDING>). Therefore, the Court must determine whether the commentary in USSG § 2B1.1(b)(9)(A) is a binding rule.\n",
      "\n",
      "The Court has held that the commentary in USSG § 2B1.1(b)(9)(A) is not a binding rule. United States v. Garcia, 24 F.3d 1120, 1123 (5th Cir.1994). The Court has also held that the commentary is not a binding rule in cases where the defendant was not acting on behalf of a charitable organization. United States v. Torres, 963 F.2d 1219, 1222 (7th Cir.1992). Therefore, the Court must apply the text of the guideline in determining the appropriate sentence.\n",
      "\n",
      "The text of USSG § 2B1.1(b)(9)(A) provides for an enhancement if the defendant was acting to obtain a benefit on behalf of a char\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \" was acting to obtain a benefit on behalf of a charitable ... organization.” U.S.S.G. § 2B1.1 cmt. 8(B). As the district court saw it and as the government sees it, Webster deserves the enhancement. He pretended to “act[ ] on behalf of a charitable ... organization,” U.S.S.G. § 2Bl.l(b)(9)(A), when he solicited personal information from the victims on behalf of fake charities. As Webster sees it, the enhancement does not apply. In his view, the commentary limits the application of the charity enhancement, and he was not acting to obtain a benefit on behalf of a charitable organization (as the commentary seems to require). As a general matter, the text of a guideline trumps commentary about it. See Stinson v. United States, 508 U.S. 36, 38, 113 S.Ct. 1913, 123 L.Ed.2d 598 (1993) (<HOLDING>). But we need not resolve whether the\"\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer, max_length=800)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/ggoseto/LEModel_3', endpoint='https://huggingface.co', repo_type='model', repo_id='ggoseto/LEModel_3')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "create_repo(\"ggoseto/LEModel_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\warpspace\\.conda\\envs\\nlp118\\Lib\\site-packages\\transformers\\utils\\hub.py:831: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4c45e4ac9645238d36d7b928f5bcc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb4892e754649128599f0ccd8b423d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c59b1fba6e49c5983598d2a063c162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/7.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf03af90baf43a2be55a43d1adeeb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\warpspace\\.conda\\envs\\nlp118\\Lib\\site-packages\\transformers\\utils\\hub.py:831: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ggoseto/LEModel_3/commit/1439519086ef7f8bc5efd2d25c309a5685379081', commit_message='Upload tokenizer', commit_description='', oid='1439519086ef7f8bc5efd2d25c309a5685379081', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTH_TOKEN = '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'\n",
    "\n",
    "REPO_NAME = 'ggoseto/LEModel_3' \n",
    "AUTH_TOKEN = AUTH_TOKEN \n",
    "\n",
    "merged_model.push_to_hub(\n",
    "    REPO_NAME, \n",
    "    use_temp_dir=True, \n",
    "    use_auth_token=AUTH_TOKEN\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    REPO_NAME, \n",
    "    use_temp_dir=True, \n",
    "    use_auth_token=AUTH_TOKEN\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
